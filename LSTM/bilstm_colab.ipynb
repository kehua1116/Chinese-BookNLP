{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bilstm_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYaXBIoiY8DW"
      },
      "source": [
        "##BiLSTM Approach on Chinese NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEPSkPbSAY1V",
        "outputId": "70bda5db-ae50-49fe-965b-d08ba7268a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# if this cell prints \"Running on cpu\", you must switch runtime environments\n",
        "# go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHSyyxQkuYb"
      },
      "source": [
        "#We install seqeval to evaluate our model and get a f1 score. \n",
        "# !pip install seqeval[gpu]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZiCDRNNAJAQ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib\n",
        "import sys, re\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "from seqeval.metrics import accuracy_score\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpMbmXFZKCw"
      },
      "source": [
        "###Character Level Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7zRH0bbE4Y2"
      },
      "source": [
        "We ontained character level embedding from this site: https://github.com/jiesutd/LatticeLSTM, which is used by this paper: https://www.aclweb.org/anthology/P18-1144/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-j02qXRAJAZ"
      },
      "source": [
        "def read_embeddings(filename, vocab_size=10000):\n",
        "    # get the embedding size from the first embedding\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        word_embedding_dim = len(file.readline().split(\" \")) - 2\n",
        "\n",
        "    vocab = {}\n",
        "\n",
        "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            if idx >= vocab_size:\n",
        "                break\n",
        "            cols = line.rstrip().split(\" \")\n",
        "            val = np.array(cols[1:])\n",
        "            char = cols[0]\n",
        "            embeddings[idx] = val\n",
        "            vocab[char] = idx\n",
        "\n",
        "    return torch.FloatTensor(embeddings), vocab"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qx2Sz_-AJAj"
      },
      "source": [
        "# this loads the 10,000 most common char embeddings\n",
        "vocab_size = 10000\n",
        "embeddings, vocab = read_embeddings('gigaword_chn.all.a2b.uni.ite50.txt', vocab_size)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VK_gYLMZSHP"
      },
      "source": [
        "###Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T87McSBVhTf9"
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self, filename):\n",
        "        \n",
        "        self.chars, self.tags = self.read_data(filename)\n",
        "        self.chars_chunk = []\n",
        "        self.tags_chunk = []\n",
        "\n",
        "\n",
        "    def read_data(self, book):\n",
        "        \"\"\"\n",
        "        Utility function, loads text file into a list of character and tag strings\n",
        "\n",
        "        Returns:\n",
        "        - chars:    a list of characters\n",
        "        - tags:         a list of tags for each character, where tags[i] contains\n",
        "                        a list of tags (strings) that correspond to the chars in \n",
        "                        chars[i]\n",
        "        \"\"\"\n",
        "        chars = []\n",
        "        tags = []\n",
        "\n",
        "        current_char = []\n",
        "        current_tags = []\n",
        "\n",
        "        book = book.split(\"\\n\")\n",
        "        \n",
        "        for line in book:\n",
        "            if line == \"\":\n",
        "                # print(\"!!!!!\")\n",
        "                if len(current_char) != 0:\n",
        "                    # print(current_char)\n",
        "                    # print(current_tags)\n",
        "                    chars.append(current_char)\n",
        "                    tags.append(current_tags)\n",
        "                    \n",
        "                current_char = []\n",
        "                current_tags = []\n",
        "            else:\n",
        "                columns = line.rstrip().split('\\t')\n",
        "                char = columns[0].lower()\n",
        "                tag = columns[1]\n",
        "                \n",
        "                current_char.append(char)\n",
        "                current_tags.append(tag)\n",
        "    \n",
        "        return chars, tags\n",
        "    \n",
        "        \n",
        "\n",
        "    def get_batches(self, batch_size, vocab, tagset, start, end, omit):\n",
        "        \"\"\"\n",
        "\n",
        "        Batches the data into mini-batches of size `batch_size`\n",
        "\n",
        "        Arguments:\n",
        "        - batch_size:       the desired output batch size\n",
        "        - vocab:            a dictionary mapping char strings to indices\n",
        "        - tagset:           a dictionary mapping tag strings to indices\n",
        "\n",
        "        Outputs:\n",
        "\n",
        "        if is_labeled=True:\n",
        "        - batched_char_indices:     a list of matrices of dimension (batch_size x max_seq_len)\n",
        "        - batched_tag_indices:      a list of matrices of dimension (batch_size x max_seq_len)\n",
        "        - batched_lengths:          a list of arrays of length (batch_size)\n",
        "\n",
        "        batched_char_indices[b] is a (batch_size x max_seq_len) matrix of integers, \n",
        "        containing index representations for characters in the b-th batch in the document. \n",
        "\n",
        "        batched_lengths[b] is a vector of length (batch_size). batched_lengths[b][i] \n",
        "        contains the original char length *before* padding for the i-th char in the currrent batch. \n",
        "\n",
        "        \"\"\"\n",
        "        PAD_INDEX = 0             # reserved for padding words, all chars shorter than max_seq_len should be padded on the right with PAD_INDEX (0).\n",
        "        UNKNOWN_INDEX = 1         # reserved for unknown words, if a char is not in the vocabulary, it gets mapped to UNKNOWN_INDEX (1).\n",
        "        IGNORE_TAG_INDEX = -100   # reserved for padding tags, all tag lists shorter than `max_seq_len` are padded with IGNORE_TAG_INDEX (-100).\n",
        "       \n",
        "        batched_char_indices = []\n",
        "        batched_tag_indices = []\n",
        "        batched_lengths = []\n",
        "        \n",
        "        if omit == -1: # for dev sets\n",
        "            chars = self.chars_chunk[start:end][0]\n",
        "            tags = self.tags_chunk[start:end][0]\n",
        "        else: # for training sets\n",
        "            chars = [self.chars_chunk[i] for i in range(start, end) if i != omit]\n",
        "            tags = [self.tags_chunk[i] for i in range(start, end) if i != omit]\n",
        "            \n",
        "            temp_char = []\n",
        "            temp_tag = []\n",
        "            for i in range(1, len(chars)):\n",
        "                temp_char += chars[i]\n",
        "                temp_tag += tags[i]\n",
        "            chars = temp_char\n",
        "            tags = temp_tag\n",
        "\n",
        "        for num_batch in range(math.ceil(len(chars) / batch_size)):\n",
        "            char_list = np.array(chars[num_batch * batch_size : min((num_batch + 1) * batch_size, len(chars))])\n",
        "            #batched_lengths\n",
        "            length_array = np.zeros(len(char_list))\n",
        "            #batched_char_indices\n",
        "            max_seq_len = len(max(char_list, key=len))\n",
        "            matrix = np.zeros((min(batch_size, len(char_list)), max_seq_len))\n",
        "            for i in range(len(char_list)):\n",
        "                matrix[i] = [vocab[word] if word in vocab else UNKNOWN_INDEX for word in char_list[i]] + [PAD_INDEX for i in range(max_seq_len - len(char_list[i]))]\n",
        "                length_array[i] = len(char_list[i])\n",
        "            batched_char_indices.append(matrix)\n",
        "            batched_lengths.append(length_array)\n",
        "\n",
        "\n",
        "        #batched_tag_indices\n",
        "        for num_batch in range(math.ceil(len(tags) / batch_size)):\n",
        "            tag_list = np.array(tags[num_batch * batch_size : min((num_batch + 1) * batch_size, len(tags))])\n",
        "            max_seq_len = len(max(tag_list, key=len))\n",
        "            matrix = np.zeros((min(batch_size, len(tag_list)), max_seq_len))\n",
        "            for i in range(len(tag_list)):\n",
        "                matrix[i] = [tagset[word] if word in tagset else UNKNOWN_INDEX for word in tag_list[i]] + [IGNORE_TAG_INDEX for i in range(max_seq_len - len(tag_list[i]))]\n",
        "            batched_tag_indices.append(matrix)\n",
        "\n",
        "            \n",
        "        return batched_char_indices, batched_tag_indices, batched_lengths\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M9n2HX4AJA3"
      },
      "source": [
        "def read_tagset(tag_file):\n",
        "    \"\"\"\n",
        "    Utility function, loads tag file into a dictionary from tag string to tag index\n",
        "    \"\"\"\n",
        "    tagset = {}\n",
        "    with open(tag_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            columns = line.rstrip().split('\\t')\n",
        "            tag = columns[0]\n",
        "            tag_id = int(columns[1])\n",
        "            tagset[tag] = tag_id\n",
        "\n",
        "    return tagset"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct1kHI-cAJBp"
      },
      "source": [
        "def combine(folder):\n",
        "    filenames = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
        "    \n",
        "    out = \"\"\n",
        "    for i in range(len(filenames)):\n",
        "        with open(folder + '/' + filenames[i], encoding='utf-8') as infile: \n",
        "            content = infile.read()\n",
        "            out = out + content\n",
        "        out = out + \"\\n\"\n",
        "    return out"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeRYj0ciAJBw"
      },
      "source": [
        "# read the files\n",
        "# all_books = combine('../data/correct_BIO_output')\n",
        "\n",
        "# with open('../data/all_books.txt', 'w', encoding='utf-8') as outfile:\n",
        "#     outfile.write(all_books) \n",
        "    \n",
        "tagset = read_tagset('NER_labels.txt')\n",
        "with open(\"all_books.txt\" ,encoding='utf-8') as file:\n",
        "  all_books = file.read()\n",
        "\n",
        "dataset = Dataset(all_books)\n",
        "\n",
        "BATCH_SIZE = 25 #for stocastic gradient descent purpose\n",
        "\n",
        "# train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "# dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
        "# test_batch_idx, test_batch_tags, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgH1F6II4yp5",
        "outputId": "e058dc89-4554-4bae-fef9-67387d749455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(dataset.chars))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBAfL9IrZY2Y"
      },
      "source": [
        "### BiLSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7dwF8XdAJCM"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    An LSTM model for sequence labeling\n",
        "\n",
        "    Initialization Arguments:\n",
        "    - embeddings:   a matrix of size (vocab_size, emb_dim)\n",
        "                  containing pretrained embedding weights\n",
        "    - hidden_dim:   the LSTM's hidden layer size\n",
        "    - tagset_size:  the number of possible output tags\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, hidden_dim, tagset_size, bidirectional_flag = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        if bidirectional_flag:\n",
        "          self.hidden_dim = hidden_dim // 2\n",
        "    \n",
        "        self.num_labels = tagset_size\n",
        "\n",
        "        vocab_size = len(embeddings)\n",
        "        embedding_dim = len(embeddings[0])\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embeddings.weight = nn.Parameter(embeddings)\n",
        "        \n",
        "        # Initialize an LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional = bidirectional_flag, batch_first=True, dropout=0.5, num_layers=3)\n",
        "\n",
        "        # Initialize a single feedforward layer\n",
        "        self.feedlayer = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, indices, lengths):\n",
        "        \"\"\"\n",
        "        Runs a batched sequence through the model and returns output logits\n",
        "\n",
        "        Arguments:\n",
        "        - indices:  a matrix of size (batch_size x max_seq_len)\n",
        "                    containing the word indices of sentences in the batch\n",
        "        - lengths:  a vector of size (batch_size) containing the\n",
        "                    original lengths of the sequences before padding\n",
        "\n",
        "        Output:\n",
        "        - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\n",
        "                    gives a score to each possible tag for each word\n",
        "                    in each sentence \n",
        "        \"\"\"\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # cast arrays as PyTorch data types and move to GPU memory\n",
        "        indices = torch.LongTensor(indices).to(device)\n",
        "        lengths = torch.LongTensor(lengths).to(device)\n",
        "\n",
        "        # convert word indices to word embeddings\n",
        "        embeddings = self.embeddings(indices)\n",
        "\n",
        "        # pack/pad handles variable length sequence batching\n",
        "        # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
        "        packed_input_embs = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
        "        # run input through LSTM layer\n",
        "        packed_output, _ = self.lstm(packed_input_embs)\n",
        "        # unpack sequences into original format\n",
        "        padded_output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        logits = self.feedlayer(padded_output)\n",
        "        return logits\n",
        "\n",
        "    def run_training(self, train_content, dev_content, batch_size, vocab, tagset,\n",
        "                         lr=5e-4, num_epochs=100, eval_every=5):\n",
        "        \"\"\"\n",
        "        Trains the model on the training data with a learning rate of lr\n",
        "        for num_epochs. Evaluates the model on the dev data eval_every epochs.\n",
        "\n",
        "        Arguments:\n",
        "        - train_dataset:  Dataset object containing the training data\n",
        "        - dev_dataset:    Dataset object containing the dev data\n",
        "        - batch_size:     batch size for train/dev data\n",
        "        - vocab:          a dictionary mapping word strings to indices\n",
        "        - tagset:         a dictionary mapping tag strings to indices\n",
        "        - lr:             learning rate\n",
        "        - num_epochs:     number of epochs to train for\n",
        "        - eval_every:     evaluation is run eval_every epochs\n",
        "        \"\"\"\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#         if str(device) == 'cpu':\n",
        "#             print(\"Training only supported in GPU environment\")\n",
        "#             return\n",
        "\n",
        "        # clear unreferenced data/models from GPU memory \n",
        "        torch.cuda.empty_cache()\n",
        "        # move model to GPU memory\n",
        "        self.to(device)\n",
        "\n",
        "        # set the optimizer (Adam) and loss function (CrossEnt)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "        # batch training and dev data\n",
        "        train_batch_idx, train_batch_tags, train_batch_lens = train_content\n",
        "        dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_content\n",
        "\n",
        "        print(\"**** TRAINING *****\")\n",
        "        for i in range(num_epochs):\n",
        "            # sets the model in train mode\n",
        "            self.train()\n",
        "\n",
        "            total_loss = 0\n",
        "            for b in range(len(train_batch_idx)):\n",
        "                # compute the logits\n",
        "                logits = self.forward(train_batch_idx[b], train_batch_lens[b])\n",
        "                # move labels to GPU memory\n",
        "                labels = torch.LongTensor(train_batch_tags[b]).to(device)\n",
        "                # compute the loss with respect to true labels\n",
        "                loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\n",
        "                total_loss += loss\n",
        "                # propagate gradients backward\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                # set model gradients to zero before performing next forward pass\n",
        "                self.zero_grad()\n",
        "\n",
        "            # print(\"Epoch{}\".format(i))\n",
        "\n",
        "            if (i + 1) % eval_every == 0:\n",
        "                # print(\"**** EVALUATION *****\")\n",
        "                # sets the model in evaluate mode (no gradients)\n",
        "                self.eval()\n",
        "                # compute dev f1 score\n",
        "                true, pred = self.evaluate(dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset)\n",
        "                \n",
        "        return true, pred\n",
        "\n",
        "                \n",
        "                \n",
        "    def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\n",
        "        \"\"\"\n",
        "        Evaluate the model's predictions on the provided dataset. \n",
        "\n",
        "        Arguments:\n",
        "        - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\n",
        "                              containing the word indices of sentences in the batch\n",
        "        - batched_lengths:    a list of vectors, each of size (batch_size), containing the\n",
        "                              original lengths of the sequences before padding\n",
        "        - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\n",
        "                              containing the tag indices corresponding to sentences in the batch\n",
        "        - num_tags:           the number of possible output tags\n",
        "\n",
        "        Output:\n",
        "        - accuracy:           the model's prediction accuracy\n",
        "        - all_true_labels:    a flattened list of all true labels\n",
        "        - all_predictions:    a flattened list of all of the model's corresponding predictions\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        all_true_labels = []\n",
        "        all_predictions = []\n",
        "\n",
        "        for b in range(len(batched_sentences)):\n",
        "            logits = self.forward(batched_sentences[b], batched_lengths[b])\n",
        "            batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "            batch_size, _ = batched_sentences[b].shape\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                tags = batched_labels[b][i]\n",
        "                preds = batch_predictions[i]\n",
        "                seq_len = int(batched_lengths[b][i])\n",
        "\n",
        "                all_true_labels.append(list(tags[:seq_len]))\n",
        "                all_predictions.append(list(preds[:seq_len]))\n",
        "\n",
        "        return all_true_labels, all_predictions, "
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfrmBNY-s5G0"
      },
      "source": [
        "def tagset_to_bio(arr):\n",
        "    \"\"\"\n",
        "    conver back from tack integer to tag string for evaluation purpose\n",
        "    \"\"\"\n",
        "    reversed_tagset = {v: k for k,v in tagset.items()}\n",
        "    for i in range(len(arr)):\n",
        "      for j in range(len(arr[i])):\n",
        "        arr[i][j] = reversed_tagset[arr[i][j]]\n",
        "\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWFKcpbPZfr_"
      },
      "source": [
        "### Model Training & 10-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85IXytabAJCu",
        "outputId": "178547ac-6907-4856-8f81-324ea0119fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Perform K-fold Cross Validation and train the model\n",
        "np.random.seed(100) \n",
        "shuffle = np.random.permutation(range(len(dataset.chars)))\n",
        "\n",
        "chars = [dataset.chars[i] for i in shuffle]\n",
        "length = int(len(chars) / 12)\n",
        "tags = [dataset.tags[i] for i in shuffle]\n",
        "dataset.tags_chunk = []\n",
        "dataset.chars_chunk = []\n",
        "\n",
        "\n",
        "#Divide data into 10 chunks\n",
        "for k in range(12):\n",
        "    if k != 11:\n",
        "        fold_chars = chars[length*k : length*(k + 1)]\n",
        "        fold_tags = tags[length*k : length*(k + 1)]\n",
        "        dataset.chars_chunk.append(fold_chars)\n",
        "        dataset.tags_chunk.append(fold_tags)\n",
        "    else:\n",
        "        fold_chars = chars[length*k :]\n",
        "        fold_tags = tags[length*k :]\n",
        "        dataset.chars_chunk.append(fold_chars)\n",
        "        dataset.tags_chunk.append(fold_tags)\n",
        "        \n",
        "\n",
        "#Loop 10 times, each time training with different chunks\n",
        "HIDDEN_SIZE = 128\n",
        "VALIDATION_FOLDS = 10\n",
        "accuracy = []\n",
        "per_class_accuracy = []\n",
        "models = []\n",
        "true_array = []\n",
        "pred_array = []\n",
        "print(\"============ K-Fold ============\")\n",
        "for k in range(VALIDATION_FOLDS):\n",
        "    print(\"#Fold: {}\".format(k))\n",
        "    train_batch_idx, train_batch_tags, train_batch_lens = dataset.get_batches(BATCH_SIZE, vocab, tagset, 0, 10, k)\n",
        "    dev_batch_idx, dev_batch_tags, dev_batch_lens = dataset.get_batches(BATCH_SIZE, vocab, tagset, k, k + 1, -1)\n",
        "    train_content = (train_batch_idx, train_batch_tags, train_batch_lens)\n",
        "    dev_content = (dev_batch_idx, dev_batch_tags, dev_batch_lens)\n",
        "    \n",
        "    # intialize a new BiLSTM model\n",
        "    model = BiLSTM(embeddings, HIDDEN_SIZE, len(tagset))\n",
        "    # train the model\n",
        "    true, pred, = model.run_training(train_content, dev_content, BATCH_SIZE, vocab, tagset,   \n",
        "                       lr=5e-4, num_epochs=25, eval_every=5)\n",
        "    models.append(model)\n",
        "    tagset_to_bio(true)\n",
        "    tagset_to_bio(pred)\n",
        "\n",
        "    true_array = np.append(true_array, true)\n",
        "    pred_array = np.append(pred_array, pred)\n",
        "    f1 = f1_score(true, pred)\n",
        "    accuracy.append(f1)\n",
        "    per_class_accuracy.append(classification_report(true, pred).split(\"\\n\"))\n",
        "    print(\"f1_score: \", f1)\n",
        "    print(\"================================\")\n",
        "    \n",
        "    \n",
        "test_batch_idx, test_batch_tags, test_batch_lens = dataset.get_batches(BATCH_SIZE, vocab, tagset, 10, 12, -1)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============ K-Fold ============\n",
            "#Fold: 0\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6369593709043251\n",
            "================================\n",
            "#Fold: 1\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6181369524984577\n",
            "================================\n",
            "#Fold: 2\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6179987413467589\n",
            "================================\n",
            "#Fold: 3\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6253041362530414\n",
            "================================\n",
            "#Fold: 4\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6570048309178744\n",
            "================================\n",
            "#Fold: 5\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6326662362814718\n",
            "================================\n",
            "#Fold: 6\n",
            "**** TRAINING *****\n",
            "f1_score:  0.5966016362492133\n",
            "================================\n",
            "#Fold: 7\n",
            "**** TRAINING *****\n",
            "f1_score:  0.5866477272727273\n",
            "================================\n",
            "#Fold: 8\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6140552995391705\n",
            "================================\n",
            "#Fold: 9\n",
            "**** TRAINING *****\n",
            "f1_score:  0.6154684095860566\n",
            "================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Rz8Z8MZms4"
      },
      "source": [
        "### Validation Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-0p4VuH3NCy",
        "outputId": "3e5b5855-62bb-4236-85e4-00010eaaedd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compute final precision/recall/f1-score\n",
        "f1_score(true_array, pred_array)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6187238258206872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEcokEuXYsyM",
        "outputId": "c7fa7ef9-5712-4176-8298-bf739b84c0e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classification_report(true_array, pred_array).split(\"\\n\")"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['              precision    recall  f1-score   support',\n",
              " '',\n",
              " '         ART       0.60      0.56      0.58       380',\n",
              " '       EVENT       0.31      0.22      0.26        73',\n",
              " '         FAC       0.32      0.28      0.30       382',\n",
              " '         GPE       0.59      0.54      0.56       771',\n",
              " '    LANGUAGE       0.12      0.12      0.12        42',\n",
              " '         LAW       0.00      0.00      0.00         2',\n",
              " '         LOC       0.29      0.27      0.28       421',\n",
              " '        NORP       0.46      0.60      0.52       516',\n",
              " '         ORG       0.23      0.17      0.19       192',\n",
              " '         PER       0.69      0.67      0.68      8459',\n",
              " '     PRODUCT       0.16      0.04      0.06       104',\n",
              " '',\n",
              " '   micro avg       0.63      0.61      0.62     11342',\n",
              " '   macro avg       0.34      0.31      0.32     11342',\n",
              " 'weighted avg       0.63      0.61      0.62     11342',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrUoKwQlZqSV"
      },
      "source": [
        "### Test set Evaluation Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOJmkwBBstY"
      },
      "source": [
        "def evaluate_on_test():\n",
        "  model = models[np.argmax(accuracy)]\n",
        "  true, pred = model.evaluate(test_batch_idx, test_batch_lens, test_batch_tags, tagset)\n",
        "  tagset_to_bio(true)\n",
        "  tagset_to_bio(pred)\n",
        "  f1 = f1_score(true, pred)\n",
        "  report = classification_report(true, pred).split(\"\\n\")\n",
        "  return f1, report"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIpt9KhIlOmT",
        "outputId": "e97e090e-3319-4aae-c407-7e47cae36355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f1, report = evaluate_on_test()\n",
        "f1"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6385093167701863"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftf2u1fVlO4T",
        "outputId": "1a29ef54-b818-42fa-8d8b-5b64888cd7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "report"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['              precision    recall  f1-score   support',\n",
              " '',\n",
              " '         ART       0.59      0.62      0.61        16',\n",
              " '         FAC       0.24      0.32      0.27        19',\n",
              " '         GPE       0.70      0.76      0.73        42',\n",
              " '    LANGUAGE       0.17      0.11      0.13         9',\n",
              " '         LOC       0.25      0.24      0.24        17',\n",
              " '        NORP       0.45      0.57      0.51        42',\n",
              " '         ORG       0.00      0.00      0.00         4',\n",
              " '         PER       0.70      0.68      0.69       646',\n",
              " '     PRODUCT       0.00      0.00      0.00        18',\n",
              " '',\n",
              " '   micro avg       0.64      0.63      0.64       813',\n",
              " '   macro avg       0.34      0.37      0.35       813',\n",
              " 'weighted avg       0.64      0.63      0.63       813',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    }
  ]
}