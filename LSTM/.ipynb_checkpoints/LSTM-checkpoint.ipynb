{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib\n",
    "import sys, re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000):\n",
    "    # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 2\n",
    "\n",
    "    vocab = {}\n",
    "\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "#             if idx + 2 >= vocab_size:\n",
    "            if idx >= vocab_size:\n",
    "                break\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "#             embeddings[idx + 2] = val\n",
    "#             vocab[word] = idx + 2\n",
    "            embeddings[idx] = val\n",
    "            vocab[word] = idx\n",
    "\n",
    "    # a FloatTensor is a multidimensional matrix\n",
    "    # that contains 32-bit floats in every entry\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads the 10,000 most common word 50-dimensional embeddings\n",
    "vocab_size = 10000\n",
    "embeddings, vocab = read_embeddings('embedding/gigaword_chn.all.a2b.uni.ite50.txt', vocab_size)\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        self.sentences, self.tags = self.read_data(filename)\n",
    "        self.sentences_chunk = []\n",
    "        self.tags_chunk = []\n",
    "\n",
    "\n",
    "    def read_data(self, book):\n",
    "        \"\"\"\n",
    "        Utility function, loads text file into a list of sentence and tag strings\n",
    "\n",
    "        Arguments:\n",
    "        - filename:     path to file\n",
    "        we assume each line is formatted as \"<word>\\t<tag>\\n\"\n",
    "\n",
    "        Returns:\n",
    "        - sentences:    a list of sentences, where each sentence is a list \n",
    "                        words (strings)\n",
    "        - tags:         a list of tags for each sentence, where tags[i] contains\n",
    "                        a list of tags (strings) that correspond to the words in \n",
    "                        sentences[i]\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        tags = []\n",
    "\n",
    "        current_sentence = []\n",
    "        current_tags = []\n",
    "\n",
    "        book = book.split(\"\\n\")\n",
    "        for line in book:\n",
    "            if line == \"\":\n",
    "                if len(current_sentence) != 0:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                    \n",
    "                current_sentence = []\n",
    "                current_tags = []\n",
    "            else:\n",
    "                columns = line.rstrip().split('\\t')\n",
    "                word = columns[0].lower()\n",
    "                tag = columns[1]\n",
    "                \n",
    "                current_sentence.append(word)\n",
    "                current_tags.append(tag)\n",
    "        return sentences, tags\n",
    "    \n",
    "        \n",
    "\n",
    "    def get_batches(self, batch_size, vocab, tagset, start, end, omit):\n",
    "        \"\"\"\n",
    "\n",
    "        Batches the data into mini-batches of size `batch_size`\n",
    "\n",
    "        Arguments:\n",
    "        - batch_size:       the desired output batch size\n",
    "        - vocab:            a dictionary mapping word strings to indices\n",
    "        - tagset:           a dictionary mapping tag strings to indices\n",
    "\n",
    "        Outputs:\n",
    "\n",
    "        if is_labeled=True:\n",
    "        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\n",
    "        - batched_tag_indices:      a list of matrices of dimension (batch_size x max_seq_len)\n",
    "        - batched_lengths:          a list of arrays of length (batch_size)\n",
    "\n",
    "\n",
    "        Description: \n",
    "\n",
    "        This function partitions the data into batches of size batch_size. If the number\n",
    "        of sentences in the document is not an even multiple of batch_size, the final batch\n",
    "        will contain the remaining elements. For example, if there are 82 sentences in the \n",
    "        dataset and batch_size=32, we return a list containing two batches of size 32 \n",
    "        and one final batch of size 18.\n",
    "\n",
    "        batched_word_indices[b] is a (batch_size x max_seq_len) matrix of integers, \n",
    "        containing index representations for sentences in the b-th batch in the document. \n",
    "        The `vocab` dictionary provides the correct mapping from word strings to indices. \n",
    "        If a word is not in the vocabulary, it gets mapped to UNKNOWN_INDEX (1).\n",
    "        `max_seq_len` is the maximum sentence length among the sentences in the current batch, \n",
    "        which will vary between different batches. All sentences shorter than max_seq_len \n",
    "        should be padded on the right with PAD_INDEX (0).\n",
    "\n",
    "        If the document is labeled, we also batch the document's tags. Analogous to \n",
    "        batched_word_indices, batched_tag_indices[b] contains the index representation\n",
    "        for the tags corresponding to the sentences in the b-th batch  in the document. \n",
    "        The `tagset` dictionary provides the correct mapping from tag strings to indicies. \n",
    "        All tag lists shorter than `max_seq_len` are padded with IGNORE_TAG_INDEX (-100).\n",
    "\n",
    "        batched_lengths[b] is a vector of length (batch_size). batched_lengths[b][i] \n",
    "        contains the original sentence length *before* padding for the i-th sentence\n",
    "        in the currrent batch. \n",
    "\n",
    "        \"\"\"\n",
    "        PAD_INDEX = 0             # reserved for padding words\n",
    "        UNKNOWN_INDEX = 1         # reserved for unknown words\n",
    "        IGNORE_TAG_INDEX = -100   # reserved for padding tags\n",
    "       \n",
    "        batched_word_indices = []\n",
    "        batched_tag_indices = []\n",
    "        batched_lengths = []\n",
    "        \n",
    "        if omit == -1: # for dev sets\n",
    "            sentences = self.sentences_chunk[start:end][0]\n",
    "            tags = self.tags_chunk[start:end][0]\n",
    "            print(len(sentences))\n",
    "        else: # for training sets\n",
    "            sentences = [self.sentences_chunk[i] for i in range(start, end) if i != omit]\n",
    "#             print([ i for i in range(start, end) if i != omit])\n",
    "            tags = [self.tags_chunk[i] for i in range(start, end) if i != omit]\n",
    "            \n",
    "            temp_sen = []\n",
    "            temp_tag = []\n",
    "            for i in range(1, len(sentences)):\n",
    "                temp_sen += sentences[i]\n",
    "                temp_tag += tags[i]\n",
    "            sentences = temp_sen\n",
    "            tags = temp_tag\n",
    "#             print(len(sentences))\n",
    "#             print(len(tags))\n",
    "            \n",
    "        for num_batch in range(math.ceil(len(sentences) / batch_size)):\n",
    "            sentence_list = np.array(sentences[num_batch * batch_size : min((num_batch + 1) * batch_size, len(sentences))])\n",
    "            #batched_lengths\n",
    "            length_array = np.zeros(len(sentence_list))\n",
    "            #batched_word_indices\n",
    "            max_seq_len = len(max(sentence_list, key=len))\n",
    "            matrix = np.zeros((min(batch_size, len(sentence_list)), max_seq_len))\n",
    "            for i in range(len(sentence_list)):\n",
    "                matrix[i] = [vocab[word] if word in vocab else UNKNOWN_INDEX for word in sentence_list[i]] + [PAD_INDEX for i in range(max_seq_len - len(sentence_list[i]))]\n",
    "                length_array[i] = len(sentence_list[i])\n",
    "            batched_word_indices.append(matrix)\n",
    "            batched_lengths.append(length_array)\n",
    "\n",
    "\n",
    "        #batched_tag_indices\n",
    "        for num_batch in range(math.ceil(len(tags) / batch_size)):\n",
    "            tag_list = np.array(tags[num_batch * batch_size : min((num_batch + 1) * batch_size, len(tags))])\n",
    "            max_seq_len = len(max(tag_list, key=len))\n",
    "            matrix = np.zeros((min(batch_size, len(tag_list)), max_seq_len))\n",
    "            for i in range(len(tag_list)):\n",
    "                matrix[i] = [tagset[word] if word in tagset else UNKNOWN_INDEX for word in tag_list[i]] + [IGNORE_TAG_INDEX for i in range(max_seq_len - len(tag_list[i]))]\n",
    "            batched_tag_indices.append(matrix)\n",
    "\n",
    "            \n",
    "        return batched_word_indices, batched_tag_indices, batched_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tagset(tag_file):\n",
    "    \"\"\"\n",
    "    Utility function, loads tag file into a dictionary from tag string to tag index\n",
    "\n",
    "    Arguments:\n",
    "    - tag_file:   file location of the tagset\n",
    "\n",
    "    Outputs:\n",
    "    - tagset:     a dictionary mapping tag strings (e.g. \"VB\") to a unique index\n",
    "    \"\"\"\n",
    "    tagset = {}\n",
    "    with open(tag_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            columns = line.rstrip().split('\\t')\n",
    "            tag = columns[0]\n",
    "            tag_id = int(columns[1])\n",
    "            tagset[tag] = tag_id\n",
    "\n",
    "    return tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "    - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "    Output: \n",
    "    - confusion_matrix:   a (num_tags x num_tags) matrix of integers\n",
    "\n",
    "    confusion_matrix[i][j] = # predictions where true label\n",
    "    was i and predicted label was j\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matrix = np.zeros((num_tags, num_tags))\n",
    "\n",
    "    #############################\n",
    "    #       YOUR CODE HERE      #\n",
    "    for i in range(len(true)):\n",
    "        true_label = true[i]\n",
    "        pred_label = pred[i]\n",
    "        confusion_matrix[true_label][pred_label] += 1\n",
    "    #############################\n",
    "\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def precision(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "    - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "    Output: \n",
    "    - precision:  an array of length num_tags, where precision[i]\n",
    "                gives the precision of class i\n",
    "\n",
    "    Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "    \"\"\"\n",
    "\n",
    "    precision = np.zeros(num_tags)\n",
    "\n",
    "    #############################\n",
    "    #       YOUR CODE HERE      #\n",
    "    matrix = confusion_matrix(true, pred, num_tags)\n",
    "    for k in range(num_tags):\n",
    "        TP = matrix[k, k] #kth row: true = k\n",
    "        FP = sum(matrix[:, k]) - matrix[k, k]\n",
    "        precision[k] = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    #############################\n",
    "\n",
    "\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "    - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "    Output: \n",
    "    - recall:     an array of length num_tags, where recall[i]\n",
    "                gives the recall of class i\n",
    "\n",
    "    Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \"\"\"\n",
    "    recall = np.zeros(num_tags)\n",
    "\n",
    "    #############################\n",
    "    #       YOUR CODE HERE      #\n",
    "    matrix = confusion_matrix(true, pred, num_tags)\n",
    "    for k in range(num_tags):\n",
    "        TP = matrix[k, k] #kth row: true = k\n",
    "        FN = sum(matrix[k, :]) - matrix[k, k]\n",
    "        recall[k] = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    #############################\n",
    "\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1_score(true, pred, num_tags):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "    - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "    Output: \n",
    "    - f1:         an array of length num_tags, where f1[i]\n",
    "                gives the recall of class i\n",
    "    \"\"\"\n",
    "    f1 = np.zeros(num_tags)\n",
    "\n",
    "    #############################\n",
    "    #       YOUR CODE HERE      #\n",
    "    p = precision(true, pred, num_tags)\n",
    "    r = recall(true, pred, num_tags)\n",
    "    for k in range(num_tags):\n",
    "        f1[k] = 2 * (p[k] * r[k]) / (p[k] + r[k])\n",
    "    #############################\n",
    "\n",
    "\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_per_class(model, dataset, vocab, tagset):\n",
    "    \"\"\"\n",
    "    Prints precision, recall, and F1 for each class in the tagset\n",
    "    \"\"\"\n",
    "    # batch the data\n",
    "    batched_idx, batched_tags, batched_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "    # compute idx --> tag from tag --> idx\n",
    "    reverse_tagset = {v: k for k,v in tagset.items()}\n",
    "    # evaluate model on hold-out set\n",
    "    acc, true, pred = model.evaluate(batched_idx, batched_lens, batched_tags, tagset)\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    print(true[:200])\n",
    "    print(pred[:200])\n",
    "\n",
    "    pr = precision(true, pred, len(tagset))\n",
    "    re = recall(true, pred, len(tagset))\n",
    "    f1 = f1_score(true, pred, len(tagset))\n",
    "\n",
    "    for idx, tag in reverse_tagset.items():\n",
    "        if tag == \"B-PER\" or tag == \"I-PER\":\n",
    "            print(\"***********************\")\n",
    "            print(\"TAG: {}\".format(tag))\n",
    "            num_pred = np.sum(pred == idx)\n",
    "            num_true = np.sum(true == idx)\n",
    "            print(\"({} pred, {} true)\".format(num_pred, num_true))\n",
    "\n",
    "            print(\"PRECISION: \\t{:.3f}\".format(pr[idx]))\n",
    "            print(\"RECALL: \\t{:.3f}\".format(re[idx]))\n",
    "            print(\"F1 SCORE: \\t{:.3f}\".format(f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(folder):\n",
    "    filenames = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "    \n",
    "    out = \"\"\n",
    "    for i in range(len(filenames)):\n",
    "        with open(folder + '/' + filenames[i], encoding='utf-8') as infile: \n",
    "            content = infile.read()\n",
    "            out = out + content\n",
    "        out = out + \"\\n\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files\n",
    "all_books = combine('../data/correct_BIO_output')\n",
    "\n",
    "with open('../data/all_books.txt', 'w', encoding='utf-8') as outfile:\n",
    "    outfile.write(all_books) \n",
    "\n",
    "    \n",
    "tagset = read_tagset('../data/NER_labels.txt')\n",
    "dataset = Dataset(all_books)\n",
    "\n",
    "BATCH_SIZE = 32 #for stocastic gradient descent purpose\n",
    "\n",
    "\n",
    "# train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "# dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "# test_batch_idx, test_batch_tags, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8867"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "738\n",
      "749\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 3, 4, 5, 6, 7, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 2, 4, 5, 6, 7, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 2, 3, 5, 6, 7, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 2, 3, 4, 6, 7, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 2, 3, 4, 5, 6, 8, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 9]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "5904\n",
      "5904\n",
      "738\n",
      "738\n"
     ]
    }
   ],
   "source": [
    "#Perform K-fold Cross Validation and train the model\n",
    "np.random.seed(159) \n",
    "shuffle = np.random.permutation(range(len(dataset.sentences)))\n",
    "\n",
    "sentences = [dataset.sentences[i] for i in shuffle]\n",
    "length = int(len(sentences) / 12)\n",
    "tags = [dataset.tags[i] for i in shuffle]\n",
    "dataset.tags_chunk = []\n",
    "dataset.sentences_chunk = []\n",
    "\n",
    "\n",
    "#Divide data into 10 chunks\n",
    "for k in range(12):\n",
    "    if k != 11:\n",
    "        fold_sentences = sentences[length*k : length*(k + 1)]\n",
    "        fold_tags = tags[length*k : length*(k + 1)]\n",
    "        dataset.sentences_chunk.append(fold_sentences)\n",
    "        dataset.tags_chunk.append(fold_tags)\n",
    "    else:\n",
    "        fold_sentences = sentences[length*k :]\n",
    "        fold_tags = tags[length*k :]\n",
    "        dataset.sentences_chunk.append(fold_sentences)\n",
    "        dataset.tags_chunk.append(fold_tags)\n",
    "#     print(len(fold_sentences))\n",
    "        \n",
    "\n",
    "#Loop 10 times, each time training with different chunks\n",
    "accuracy = []\n",
    "HIDDEN_SIZE = 64\n",
    "for k in range(10):\n",
    "    train_batch_idx, train_batch_tags, train_batch_lens = dataset.get_batches(BATCH_SIZE, vocab, tagset, 0, 10, k)\n",
    "    dev_batch_idx, dev_batch_tags, dev_batch_lens = dataset.get_batches(BATCH_SIZE, vocab, tagset, k, k + 1, -1)\n",
    "    \n",
    "    \n",
    "    # intialize a new LSTMTagger model\n",
    "    model = LSTMTagger(embeddings, HIDDEN_SIZE, len(tagset))\n",
    "    # train the model\n",
    "    acc, true, pred = model.run_training(train_dataset, dev_dataset, BATCH_SIZE, vocab, tagset,   \n",
    "                       lr=5e-4, num_epochs=25, eval_every=5)\n",
    "    accuracy.append(acc)\n",
    "    eval_per_class(model, dev_dataset, vocab, tagset)\n",
    "    \n",
    "    \n",
    "test_batch_idx, test_batch_tags, test_batch_lens = dataset.get_batches(BATCH_SIZE, vocab, tagset, 10, 12, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "a[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dataset.sentences_chunk[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM model for sequence labeling\n",
    "\n",
    "    Initialization Arguments:\n",
    "    - embeddings:   a matrix of size (vocab_size, emb_dim)\n",
    "                  containing pretrained embedding weights\n",
    "    - hidden_dim:   the LSTM's hidden layer size\n",
    "    - tagset_size:  the number of possible output tags\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, hidden_dim, tagset_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = tagset_size\n",
    "\n",
    "        #############################\n",
    "        #       YOUR CODE HERE      #\n",
    "        #############################\n",
    "\n",
    "        # Initialize a PyTorch embeddings layer using the pretrained embedding weights\n",
    "        vocab_size = len(embeddings)\n",
    "        embedding_dim = len(embeddings[0])\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "        # Initialize an LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=0.3, num_layers=3)\n",
    "\n",
    "        # Initialize a single feedforward layer\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, indices, lengths):\n",
    "        \"\"\"\n",
    "        Runs a batched sequence through the model and returns output logits\n",
    "\n",
    "        Arguments:\n",
    "        - indices:  a matrix of size (batch_size x max_seq_len)\n",
    "                    containing the word indices of sentences in the batch\n",
    "        - lengths:  a vector of size (batch_size) containing the\n",
    "                    original lengths of the sequences before padding\n",
    "\n",
    "        Output:\n",
    "        - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\n",
    "                    gives a score to each possible tag for each word\n",
    "                    in each sentence \n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # cast arrays as PyTorch data types and move to GPU memory\n",
    "        indices = torch.LongTensor(indices).to(device)\n",
    "        lengths = torch.LongTensor(lengths).to(device)\n",
    "\n",
    "        # convert word indices to word embeddings\n",
    "        embeddings = self.embeddings(indices)\n",
    "\n",
    "        # pack/pad handles variable length sequence batching\n",
    "        # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        packed_input_embs = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "        # run input through LSTM layer\n",
    "        packed_output, _ = self.lstm(packed_input_embs)\n",
    "        # unpack sequences into original format\n",
    "        padded_output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        logits = self.hidden2tag(padded_output)\n",
    "        return logits\n",
    "\n",
    "    def run_training(self, train_dataset, dev_dataset, batch_size, vocab, tagset,\n",
    "                         lr=5e-4, num_epochs=100, eval_every=5):\n",
    "        \"\"\"\n",
    "        Trains the model on the training data with a learning rate of lr\n",
    "        for num_epochs. Evaluates the model on the dev data eval_every epochs.\n",
    "\n",
    "        Arguments:\n",
    "        - train_dataset:  Dataset object containing the training data\n",
    "        - dev_dataset:    Dataset object containing the dev data\n",
    "        - batch_size:     batch size for train/dev data\n",
    "        - vocab:          a dictionary mapping word strings to indices\n",
    "        - tagset:         a dictionary mapping tag strings to indices\n",
    "        - lr:             learning rate\n",
    "        - num_epochs:     number of epochs to train for\n",
    "        - eval_every:     evaluation is run eval_every epochs\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#         if str(device) == 'cpu':\n",
    "#             print(\"Training only supported in GPU environment\")\n",
    "#             return\n",
    "\n",
    "        # clear unreferenced data/models from GPU memory \n",
    "        torch.cuda.empty_cache()\n",
    "        # move model to GPU memory\n",
    "        self.to(device)\n",
    "\n",
    "        # set the optimizer (Adam) and loss function (CrossEnt)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "        # batch training and dev data\n",
    "        train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "        dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "\n",
    "        print(\"**** TRAINING *****\")\n",
    "        for i in range(num_epochs):\n",
    "            # sets the model in train mode\n",
    "            self.train()\n",
    "\n",
    "            total_loss = 0\n",
    "            for b in range(len(train_batch_idx)):\n",
    "                # compute the logits\n",
    "                logits = self.forward(train_batch_idx[b], train_batch_lens[b])\n",
    "                # move labels to GPU memory\n",
    "                labels = torch.LongTensor(train_batch_tags[b]).to(device)\n",
    "                # compute the loss with respect to true labels\n",
    "                loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\n",
    "                total_loss += loss\n",
    "                # propagate gradients backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # set model gradients to zero before performing next forward pass\n",
    "                self.zero_grad()\n",
    "\n",
    "            print(\"Epoch {} | Loss: {}\".format(i, total_loss))\n",
    "\n",
    "            if (i + 1) % eval_every == 0:\n",
    "                print(\"**** EVALUATION *****\")\n",
    "                # sets the model in evaluate mode (no gradients)\n",
    "                self.eval()\n",
    "                # compute dev f1 score\n",
    "                acc, true, pred = self.evaluate(dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset)\n",
    "                print(\"Dev Accuracy: {}\".format(acc))\n",
    "                print(\"**********************\")\n",
    "                \n",
    "        return acc, true, pred\n",
    "\n",
    "                \n",
    "                \n",
    "    def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\n",
    "        \"\"\"\n",
    "        Evaluate the model's predictions on the provided dataset. \n",
    "\n",
    "        Arguments:\n",
    "        - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                              containing the word indices of sentences in the batch\n",
    "        - batched_lengths:    a list of vectors, each of size (batch_size), containing the\n",
    "                              original lengths of the sequences before padding\n",
    "        - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                              containing the tag indices corresponding to sentences in the batch\n",
    "        - num_tags:           the number of possible output tags\n",
    "\n",
    "        Output:\n",
    "        - accuracy:           the model's prediction accuracy\n",
    "        - all_true_labels:    a flattened list of all true labels\n",
    "        - all_predictions:    a flattened list of all of the model's corresponding predictions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        all_true_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for b in range(len(batched_sentences)):\n",
    "            logits = self.forward(batched_sentences[b], batched_lengths[b])\n",
    "            batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            batch_size, _ = batched_sentences[b].shape\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                tags = batched_labels[b][i]\n",
    "                preds = batch_predictions[i]\n",
    "\n",
    "                seq_len = int(batched_lengths[b][i])\n",
    "                for j in range(seq_len):\n",
    "                    all_predictions.append(int(preds[j]))\n",
    "                    all_true_labels.append(int(tags[j]))\n",
    "\n",
    "\n",
    "        acc = accuracy(all_true_labels, all_predictions)\n",
    "\n",
    "        return acc, all_true_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, pred):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - true:       a list of true label values (integers)\n",
    "    - pred:       a list of predicted label values (integers)\n",
    "\n",
    "    Output:\n",
    "    - accuracy:   the prediction accuracy\n",
    "    \"\"\"\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    num_correct = sum(true == pred)\n",
    "    num_total = len(true)\n",
    "\n",
    "    return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(159)\n",
    "\n",
    "# HIDDEN_SIZE = 64\n",
    "# # intialize a new LSTMTagger model\n",
    "# model = LSTMTagger(embeddings, HIDDEN_SIZE, len(tagset))\n",
    "# # train the model\n",
    "# model.run_training(train_dataset, dev_dataset, BATCH_SIZE, vocab, tagset,   \n",
    "#                    lr=5e-4, num_epochs=25, eval_every=5)\n",
    "# eval_per_class(model, dev_dataset, vocab, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_per_class(model, dev_dataset, vocab, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
