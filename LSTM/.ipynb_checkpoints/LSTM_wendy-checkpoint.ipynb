{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib\n",
    "import sys, re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embeddings(filename, vocab_size=10000):\n",
    "    # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 2\n",
    "\n",
    "    vocab = {}\n",
    "\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "#             if idx + 2 >= vocab_size:\n",
    "            if idx >= vocab_size:\n",
    "                break\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "#             embeddings[idx + 2] = val\n",
    "#             vocab[word] = idx + 2\n",
    "            embeddings[idx] = val\n",
    "            vocab[word] = idx\n",
    "\n",
    "    # a FloatTensor is a multidimensional matrix\n",
    "    # that contains 32-bit floats in every entry\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.0050e-03,  8.8390e-03, -7.6610e-03,  ...,  3.3940e-03,\n",
      "          4.0300e-04,  2.6620e-03],\n",
      "        [-5.7535e+00, -4.0850e+00,  1.0009e+01,  ..., -2.0161e+00,\n",
      "          9.9122e-02,  6.3788e+00],\n",
      "        [-6.8784e+00, -8.6876e-01,  5.9120e+00,  ..., -2.0462e-01,\n",
      "         -8.4556e-01,  3.0137e+00],\n",
      "        ...,\n",
      "        [-2.5837e-01, -1.0052e+00, -1.0706e-01,  ..., -1.9379e-01,\n",
      "         -9.7554e-01,  1.3865e+00],\n",
      "        [ 2.7814e-01, -2.3732e-01,  1.7664e-01,  ..., -2.9520e-02,\n",
      "          2.0987e-02,  1.0988e+00],\n",
      "        [ 1.8454e-01,  4.1022e-01,  1.0133e+00,  ..., -6.7884e-01,\n",
      "          3.5332e-01, -6.9508e-01]])\n"
     ]
    }
   ],
   "source": [
    "# this loads the 10,000 most common word 50-dimensional embeddings\n",
    "vocab_size = 10000\n",
    "embeddings, vocab = read_embeddings('embedding/gigaword_chn.all.a2b.uni.ite50.txt', vocab_size)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        self.sentences, self.tags = self.read_data(filename)\n",
    "\n",
    "\n",
    "    def read_data(self, filename):\n",
    "        \"\"\"\n",
    "        Utility function, loads text file into a list of sentence and tag strings\n",
    "\n",
    "        Arguments:\n",
    "        - filename:     path to file\n",
    "        we assume each line is formatted as \"<word>\\t<tag>\\n\"\n",
    "\n",
    "        Returns:\n",
    "        - sentences:    a list of sentences, where each sentence is a list \n",
    "                        words (strings)\n",
    "        - tags:         a list of tags for each sentence, where tags[i] contains\n",
    "                        a list of tags (strings) that correspond to the words in \n",
    "                        sentences[i]\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        tags = []\n",
    "\n",
    "        current_sentence = []\n",
    "        current_tags = []\n",
    "\n",
    "        with open(filename, encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                assert (\"\\t\" in line and \"\\n\" in line) or (line == \"\\n\")\n",
    "\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                if line == '\\n':\n",
    "                    if len(current_sentence) != 0:\n",
    "                        sentences.append(current_sentence)\n",
    "                        tags.append(current_tags)\n",
    "\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "                else:\n",
    "                    columns = line.rstrip().split('\\t')\n",
    "                    word = columns[0].lower()\n",
    "                    tag = columns[1]\n",
    "\n",
    "                    current_sentence.append(word)\n",
    "                    current_tags.append(tag)\n",
    "\n",
    "            return sentences, tags\n",
    "\n",
    "\n",
    "    def get_batches(self, batch_size, vocab, tagset):\n",
    "        \"\"\"\n",
    "\n",
    "        Batches the data into mini-batches of size `batch_size`\n",
    "\n",
    "        Arguments:\n",
    "        - batch_size:       the desired output batch size\n",
    "        - vocab:            a dictionary mapping word strings to indices\n",
    "        - tagset:           a dictionary mapping tag strings to indices\n",
    "\n",
    "        Outputs:\n",
    "\n",
    "        if is_labeled=True:\n",
    "        - batched_word_indices:     a list of matrices of dimension (batch_size x max_seq_len)\n",
    "        - batched_tag_indices:      a list of matrices of dimension (batch_size x max_seq_len)\n",
    "        - batched_lengths:          a list of arrays of length (batch_size)\n",
    "\n",
    "\n",
    "        Description: \n",
    "\n",
    "        This function partitions the data into batches of size batch_size. If the number\n",
    "        of sentences in the document is not an even multiple of batch_size, the final batch\n",
    "        will contain the remaining elements. For example, if there are 82 sentences in the \n",
    "        dataset and batch_size=32, we return a list containing two batches of size 32 \n",
    "        and one final batch of size 18.\n",
    "\n",
    "        batched_word_indices[b] is a (batch_size x max_seq_len) matrix of integers, \n",
    "        containing index representations for sentences in the b-th batch in the document. \n",
    "        The `vocab` dictionary provides the correct mapping from word strings to indices. \n",
    "        If a word is not in the vocabulary, it gets mapped to UNKNOWN_INDEX (1).\n",
    "        `max_seq_len` is the maximum sentence length among the sentences in the current batch, \n",
    "        which will vary between different batches. All sentences shorter than max_seq_len \n",
    "        should be padded on the right with PAD_INDEX (0).\n",
    "\n",
    "        If the document is labeled, we also batch the document's tags. Analogous to \n",
    "        batched_word_indices, batched_tag_indices[b] contains the index representation\n",
    "        for the tags corresponding to the sentences in the b-th batch  in the document. \n",
    "        The `tagset` dictionary provides the correct mapping from tag strings to indicies. \n",
    "        All tag lists shorter than `max_seq_len` are padded with IGNORE_TAG_INDEX (-100).\n",
    "\n",
    "        batched_lengths[b] is a vector of length (batch_size). batched_lengths[b][i] \n",
    "        contains the original sentence length *before* padding for the i-th sentence\n",
    "        in the currrent batch. \n",
    "\n",
    "        \"\"\"\n",
    "        PAD_INDEX = 0             # reserved for padding words\n",
    "        UNKNOWN_INDEX = 1         # reserved for unknown words\n",
    "        IGNORE_TAG_INDEX = -100   # reserved for padding tags\n",
    "\n",
    "        # randomly shuffle the data\n",
    "        np.random.seed(159) # DON'T CHANGE THIS\n",
    "        shuffle = np.random.permutation(range(len(self.sentences)))\n",
    "\n",
    "        sentences = [self.sentences[i] for i in shuffle]\n",
    "        tags = [self.tags[i] for i in shuffle]\n",
    "\n",
    "\n",
    "        batched_word_indices = []\n",
    "        batched_tag_indices = []\n",
    "        batched_lengths = []\n",
    "        #############################\n",
    "        #       YOUR CODE HERE      #\n",
    "\n",
    "        #############################\n",
    "\n",
    "        for num_batch in range(math.ceil(len(sentences) / batch_size)):\n",
    "            sentence_list = np.array(sentences[num_batch * batch_size : min((num_batch + 1) * batch_size, len(sentences))])\n",
    "            #batched_lengths\n",
    "            length_array = np.zeros(len(sentence_list))\n",
    "            #batched_word_indices\n",
    "            max_seq_len = len(max(sentence_list, key=len))\n",
    "            matrix = np.zeros((min(batch_size, len(sentence_list)), max_seq_len))\n",
    "            for i in range(len(sentence_list)):\n",
    "                matrix[i] = [vocab[word] if word in vocab else UNKNOWN_INDEX for word in sentence_list[i]] + [PAD_INDEX for i in range(max_seq_len - len(sentence_list[i]))]\n",
    "                length_array[i] = len(sentence_list[i])\n",
    "            batched_word_indices.append(matrix)\n",
    "            batched_lengths.append(length_array)\n",
    "\n",
    "\n",
    "        #batched_tag_indices\n",
    "        for num_batch in range(math.ceil(len(tags) / batch_size)):\n",
    "            tag_list = np.array(tags[num_batch * batch_size : min((num_batch + 1) * batch_size, len(tags))])\n",
    "            max_seq_len = len(max(tag_list, key=len))\n",
    "            matrix = np.zeros((min(batch_size, len(tag_list)), max_seq_len))\n",
    "            for i in range(len(tag_list)):\n",
    "                matrix[i] = [tagset[word] if word in tagset else UNKNOWN_INDEX for word in tag_list[i]] + [IGNORE_TAG_INDEX for i in range(max_seq_len - len(tag_list[i]))]\n",
    "            batched_tag_indices.append(matrix)\n",
    "\n",
    "        #############################\n",
    "        #       DO NOT MODIFY       #\n",
    "        #############################\n",
    "        return batched_word_indices, batched_tag_indices, batched_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets random seeds and sets model in deterministic\n",
    "    training mode. Ensures reproducible results\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def accuracy(true, pred):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "\n",
    "  Output:\n",
    "  - accuracy:   the prediction accuracy\n",
    "  \"\"\"\n",
    "  true = np.array(true)\n",
    "  pred = np.array(pred)\n",
    "\n",
    "  num_correct = sum(true == pred)\n",
    "  num_total = len(true)\n",
    "  return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - confusion_matrix:   a (num_tags x num_tags) matrix of integers\n",
    "\n",
    "  confusion_matrix[i][j] = # predictions where true label\n",
    "  was i and predicted label was j\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  confusion_matrix = np.zeros((num_tags, num_tags))\n",
    "\n",
    "  #############################\n",
    "  #       YOUR CODE HERE      #\n",
    "  for i in range(len(true)):\n",
    "    true_label = true[i]\n",
    "    pred_label = pred[i]\n",
    "    confusion_matrix[true_label][pred_label] += 1\n",
    "\n",
    "  #############################\n",
    "\n",
    "  return confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def precision(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - precision:  an array of length num_tags, where precision[i]\n",
    "                gives the precision of class i\n",
    "\n",
    "  Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "  \"\"\"\n",
    "\n",
    "  precision = np.zeros(num_tags)\n",
    "  #############################\n",
    "  #       YOUR CODE HERE      #\n",
    "  c_matrix = confusion_matrix(true, pred, num_tags)\n",
    "  num_right = np.diag(c_matrix)\n",
    "  divider = np.sum(c_matrix, axis = 0)\n",
    "  precision = np.divide(num_right, divider, out=np.zeros_like(num_right), where = divider != 0)\n",
    "  #############################\n",
    "\n",
    "\n",
    "  \n",
    "  return precision\n",
    "\n",
    "\n",
    "def recall(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - recall:     an array of length num_tags, where recall[i]\n",
    "                gives the recall of class i\n",
    "\n",
    "  Hints:  the confusion matrix may be useful\n",
    "          be careful about zero division\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  YOUR CODE HERE\n",
    "  \"\"\"\n",
    "  recall = np.zeros(num_tags)\n",
    "\n",
    "  #############################\n",
    "  #       YOUR CODE HERE      #\n",
    "  c_matrix = confusion_matrix(true, pred, num_tags)\n",
    "  num_right = np.diag(c_matrix)\n",
    "  deno = np.sum(c_matrix, axis = 1)\n",
    "  recall = np.divide(num_right, deno, out=np.zeros_like(num_right), where = deno != 0)\n",
    "  #############################\n",
    "\n",
    "  return recall\n",
    "\n",
    "\n",
    "def f1_score(true, pred, num_tags):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  - true:       a list of true label values (integers)\n",
    "  - pred:       a list of predicted label values (integers)\n",
    "  - num_tags:   the number of possible tags\n",
    "                true and pred will both contain integers between\n",
    "                0 and num_tags - 1 (inclusive)\n",
    "\n",
    "  Output: \n",
    "  - f1:         an array of length num_tags, where f1[i]\n",
    "                gives the recall of class i\n",
    "  \"\"\"\n",
    "  f1 = np.zeros(num_tags)\n",
    "\n",
    "  #############################\n",
    "  #       YOUR CODE HERE      #\n",
    "\n",
    "  precision_score = precision(true, pred, num_tags)\n",
    "  recall_score = recall(true, pred, num_tags)\n",
    "  numerator = 2 * np.multiply(precision_score, recall_score)\n",
    "  deno = np.add(precision_score, recall_score)\n",
    "  f1 = np.divide(numerator, deno, out=np.zeros_like(numerator), where = deno != 0)\n",
    "  #############################\n",
    "\n",
    "  return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_per_class(model, dataset, vocab, tagset):\n",
    "  \"\"\"\n",
    "  Prints precision, recall, and F1 for each class in the tagset\n",
    "  \"\"\"\n",
    "  # batch the data\n",
    "  batched_idx, batched_tags, batched_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "  # compute idx --> tag from tag --> idx\n",
    "  reverse_tagset = {v: k for k,v in tagset.items()}\n",
    "  # evaluate model on hold-out set\n",
    "  acc, true, pred = model.evaluate(batched_idx, batched_lens, batched_tags, tagset)\n",
    "  true = np.array(true)\n",
    "  pred = np.array(pred)\n",
    "\n",
    "  pr = precision(true, pred, len(tagset))\n",
    "  re = recall(true, pred, len(tagset))\n",
    "  f1 = f1_score(true, pred, len(tagset))\n",
    "\n",
    "  for idx, tag in reverse_tagset.items():\n",
    "    print(\"***********************\")\n",
    "    print(\"TAG: {}\".format(tag))\n",
    "    num_pred = np.sum(pred == idx)\n",
    "    num_true = np.sum(true == idx)\n",
    "    print(\"({} pred, {} true)\".format(num_pred, num_true))\n",
    "\n",
    "    print(\"PRECISION: \\t{:.3f}\".format(pr[idx]))\n",
    "    print(\"RECALL: \\t{:.3f}\".format(re[idx]))\n",
    "    print(\"F1 SCORE: \\t{:.3f}\".format(f1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tagset(tag_file):\n",
    "    \"\"\"\n",
    "    Utility function, loads tag file into a dictionary from tag string to tag index\n",
    "\n",
    "    Arguments:\n",
    "    - tag_file:   file location of the tagset\n",
    "\n",
    "    Outputs:\n",
    "    - tagset:     a dictionary mapping tag strings (e.g. \"VB\") to a unique index\n",
    "    \"\"\"\n",
    "    tagset = {}\n",
    "    with open(tag_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            columns = line.rstrip().split('\\t')\n",
    "            tag = columns[0]\n",
    "            tag_id = int(columns[1])\n",
    "            tagset[tag] = tag_id\n",
    "\n",
    "    return tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files\n",
    "tagset = read_tagset('data/NER_labels.txt')\n",
    "# train_dataset = Dataset('data/correct_BIO_output/book_1_BIO.txt')\n",
    "# dev_dataset = Dataset('data/correct_BIO_output/book_2_BIO.txt')\n",
    "\n",
    "train_dataset = Dataset('data/train_books.txt')\n",
    "dev_dataset = Dataset('data/dev_books.txt')\n",
    "\n",
    "# test_dataset = Dataset('pos.test')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# #these should run without errors if implemented correctly\n",
    "train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "# dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "# test_batch_idx, test_batch_lens = test_dataset.get_batches(BATCH_SIZE, vocab, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training update LSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "  \"\"\"\n",
    "  An BiLSTM model for NER Tagging\n",
    "\n",
    "  Initialization Arguments:\n",
    "  - embeddings:   a matrix of size (vocab_size, emb_dim)\n",
    "                  containing pretrained embedding weights\n",
    "  - hidden_dim:   the LSTM's hidden layer size\n",
    "  - tagset_size:  the number of possible output tags\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, embeddings, hidden_dim, tagset_size, bidirectional_flag = True):\n",
    "    super().__init__()\n",
    "  \n",
    "    self.hidden_dim = hidden_dim\n",
    "    if bidirectional_flag:\n",
    "        self.hidden_dim = hidden_dim // 2\n",
    "    \n",
    "    self.num_labels = tagset_size\n",
    "\n",
    "    # Initialize a PyTorch embeddings layer using the pretrained embedding weights\n",
    "    vocab_size = len(embeddings)\n",
    "    embedding_dim = len(embeddings[0])\n",
    "\n",
    "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.embeddings.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    # Initialize an LSTM layer\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional = bidirectional_flag)\n",
    "\n",
    "    # Initialize a single feedforward layer\n",
    "    self.feedlayer = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "  \n",
    "  def forward(self, indices, lengths):\n",
    "    \"\"\"\n",
    "    Runs a batched sequence through the model and returns output logits\n",
    "\n",
    "    Arguments:\n",
    "    - indices:  a matrix of size (batch_size x max_seq_len)\n",
    "                containing the word indices of sentences in the batch\n",
    "    - lengths:  a vector of size (batch_size) containing the\n",
    "                original lengths of the sequences before padding\n",
    "\n",
    "    Output:\n",
    "    - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\n",
    "                gives a score to each possible tag for each word\n",
    "                in each sentence\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # cast arrays as PyTorch data types and move to GPU memory\n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    lengths = torch.LongTensor(lengths).to(device)\n",
    "    \n",
    "    # convert word indices to word embeddings\n",
    "    embeddings = self.embeddings(indices)\n",
    "\n",
    "    # pack/pad handles variable length sequence batching\n",
    "    # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "    packed_input_embs = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "    # run input through LSTM layer\n",
    "    packed_output, _ = self.lstm(packed_input_embs)\n",
    "    # unpack sequences into original format\n",
    "    padded_output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "    logits = self.feedlayer(padded_output)\n",
    "    return logits\n",
    "\n",
    "  def run_training(self, train_dataset, dev_dataset, batch_size, vocab, tagset,\n",
    "                         lr=5e-4, num_epochs=100, eval_every=5):\n",
    "    \"\"\"\n",
    "    Trains the model on the training data with a learning rate of lr\n",
    "    for num_epochs. Evaluates the model on the dev data eval_every epochs.\n",
    "\n",
    "    Arguments:\n",
    "    - train_dataset:  Dataset object containing the training data\n",
    "    - dev_dataset:    Dataset object containing the dev data\n",
    "    - batch_size:     batch size for train/dev data\n",
    "    - vocab:          a dictionary mapping word strings to indices\n",
    "    - tagset:         a dictionary mapping tag strings to indices\n",
    "    - lr:             learning rate\n",
    "    - num_epochs:     number of epochs to train for\n",
    "    - eval_every:     evaluation is run eval_every epochs\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # if str(device) == 'cpu':\n",
    "    #   print(\"Training only supported in GPU environment\")\n",
    "    #   return\n",
    "\n",
    "    # clear unreferenced data/models from GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    # move model to GPU memory\n",
    "    self.to(device)\n",
    "\n",
    "    # set the optimizer (Adam) and loss function (CrossEnt)\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    # batch training and dev data\n",
    "    train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "    dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "\n",
    "    print(\"**** TRAINING *****\")\n",
    "    for i in range(num_epochs):\n",
    "      # sets the model in train mode\n",
    "      self.train()\n",
    "\n",
    "      total_loss = 0\n",
    "      for b in range(len(train_batch_idx)):\n",
    "        # compute the logits\n",
    "        logits = self.forward(train_batch_idx[b], train_batch_lens[b])\n",
    "        # move labels to GPU memory\n",
    "        labels = torch.LongTensor(train_batch_tags[b]).to(device)\n",
    "        # compute the loss with respect to true labels\n",
    "        loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\n",
    "        total_loss += loss\n",
    "        # propagate gradients backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set model gradients to zero before performing next forward pass\n",
    "        self.zero_grad()\n",
    "\n",
    "      print(\"Epoch {} | Loss: {}\".format(i, total_loss))\n",
    "\n",
    "      if (i + 1) % eval_every == 0:\n",
    "        print(\"**** EVALUATION *****\")\n",
    "        # sets the model in evaluate mode (no gradients)\n",
    "        self.eval()\n",
    "        # compute dev f1 score\n",
    "        acc, true, pred = self.evaluate(dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset)\n",
    "        print(\"Dev Accuracy: {}\".format(acc))\n",
    "        print(\"**********************\")\n",
    "\n",
    "  def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\n",
    "    \"\"\"\n",
    "    Evaluate the model's predictions on the provided dataset.\n",
    "\n",
    "    Arguments:\n",
    "    - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                          containing the word indices of sentences in the batch\n",
    "    - batched_lengths:    a list of vectors, each of size (batch_size), containing the\n",
    "                          original lengths of the sequences before padding\n",
    "    - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                          containing the tag indices corresponding to sentences in the batch\n",
    "    - num_tags:           the number of possible output tags\n",
    "\n",
    "    Output:\n",
    "    - accuracy:           the model's prediction accuracy\n",
    "    - all_true_labels:    a flattened list of all true labels\n",
    "    - all_predictions:    a flattened list of all of the model's corresponding predictions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for b in range(len(batched_sentences)):\n",
    "      logits = self.forward(batched_sentences[b], batched_lengths[b])\n",
    "      batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "      batch_size, _ = batched_sentences[b].shape\n",
    "\n",
    "      for i in range(batch_size):\n",
    "        tags = batched_labels[b][i]\n",
    "        preds = batch_predictions[i]\n",
    "        \n",
    "        seq_len = int(batched_lengths[b][i])\n",
    "        for j in range(seq_len):\n",
    "          all_predictions.append(int(preds[j]))\n",
    "          all_true_labels.append(int(tags[j]))\n",
    "      \n",
    "    \n",
    "    acc = accuracy(all_true_labels, all_predictions)\n",
    "    \n",
    "    #print(all_predictions)\n",
    "      \n",
    "    return acc, all_true_labels, all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** TRAINING *****\n",
      "Epoch 0 | Loss: 36.587955474853516\n",
      "Epoch 1 | Loss: 21.898386001586914\n",
      "Epoch 2 | Loss: 9.541976928710938\n",
      "Epoch 3 | Loss: 4.72255277633667\n",
      "Epoch 4 | Loss: 3.579132318496704\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9515643592076714\n",
      "**********************\n",
      "Epoch 5 | Loss: 3.141530752182007\n",
      "Epoch 6 | Loss: 2.8433265686035156\n",
      "Epoch 7 | Loss: 2.624493360519409\n",
      "Epoch 8 | Loss: 2.451234817504883\n",
      "Epoch 9 | Loss: 2.2983899116516113\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9524742773150416\n",
      "**********************\n",
      "Epoch 10 | Loss: 2.157374143600464\n",
      "Epoch 11 | Loss: 2.027245044708252\n",
      "Epoch 12 | Loss: 1.9093087911605835\n",
      "Epoch 13 | Loss: 1.8035744428634644\n",
      "Epoch 14 | Loss: 1.7078444957733154\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9511443970042697\n",
      "**********************\n",
      "Epoch 15 | Loss: 1.6220111846923828\n",
      "Epoch 16 | Loss: 1.5450047254562378\n",
      "Epoch 17 | Loss: 1.4743824005126953\n",
      "Epoch 18 | Loss: 1.4091449975967407\n",
      "Epoch 19 | Loss: 1.3491148948669434\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9509344159025688\n",
      "**********************\n",
      "Epoch 20 | Loss: 1.293289303779602\n",
      "Epoch 21 | Loss: 1.241038203239441\n",
      "Epoch 22 | Loss: 1.1922059059143066\n",
      "Epoch 23 | Loss: 1.14633309841156\n",
      "Epoch 24 | Loss: 1.1030479669570923\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9513543781059705\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "# sets the random seed – DO NOT change this\n",
    "# this ensures deterministic results that are comparable with the staff values\n",
    "set_seed(159)\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "# intialize a new LSTMTagger model\n",
    "bilstm_model = BiLSTM(embeddings, HIDDEN_SIZE, len(tagset))\n",
    "# train the model\n",
    "bilstm_model.run_training(train_dataset, dev_dataset, BATCH_SIZE, vocab, tagset,   \n",
    "                   lr=5e-4, num_epochs=25, eval_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************\n",
      "TAG: B-PER\n",
      "(145 pred, 328 true)\n",
      "PRECISION: \t0.372\n",
      "RECALL: \t0.165\n",
      "F1 SCORE: \t0.228\n",
      "***********************\n",
      "TAG: I-PER\n",
      "(8 pred, 168 true)\n",
      "PRECISION: \t0.250\n",
      "RECALL: \t0.012\n",
      "F1 SCORE: \t0.023\n",
      "***********************\n",
      "TAG: B-NORP\n",
      "(0 pred, 14 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-NORP\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-FAC\n",
      "(0 pred, 7 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-FAC\n",
      "(0 pred, 20 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-ORG\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-ORG\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-GPE\n",
      "(0 pred, 33 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-GPE\n",
      "(0 pred, 30 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-LOC\n",
      "(0 pred, 36 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-LOC\n",
      "(0 pred, 18 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-PRODUCT\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-PRODUCT\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-EVENT\n",
      "(0 pred, 2 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-EVENT\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-ART\n",
      "(0 pred, 12 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-ART\n",
      "(0 pred, 8 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-LAW\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-LAW\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-LANGUAGE\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-LANGUAGE\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: O\n",
      "(14134 pred, 13595 true)\n",
      "PRECISION: \t0.958\n",
      "RECALL: \t0.996\n",
      "F1 SCORE: \t0.976\n"
     ]
    }
   ],
   "source": [
    "eval_per_class(bilstm_model, dev_dataset, vocab, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "  \"\"\"\n",
    "  An LSTM model for sequence labeling\n",
    "\n",
    "  Initialization Arguments:\n",
    "  - embeddings:   a matrix of size (vocab_size, emb_dim)\n",
    "                  containing pretrained embedding weights\n",
    "  - hidden_dim:   the LSTM's hidden layer size\n",
    "  - tagset_size:  the number of possible output tags\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, embeddings, hidden_dim, tagset_size):\n",
    "    super().__init__()\n",
    "  \n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_labels = tagset_size\n",
    "\n",
    "    #############################\n",
    "    #       YOUR CODE HERE      #\n",
    "    #############################\n",
    "\n",
    "    # Initialize a PyTorch embeddings layer using the pretrained embedding weights\n",
    "    vocab_size = len(embeddings)\n",
    "    embedding_dim = len(embeddings[0])\n",
    "\n",
    "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    self.embeddings.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    # Initialize an LSTM layer\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "    # Initialize a single feedforward layer\n",
    "    self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "  \n",
    "  def forward(self, indices, lengths):\n",
    "    \"\"\"\n",
    "    Runs a batched sequence through the model and returns output logits\n",
    "\n",
    "    Arguments:\n",
    "    - indices:  a matrix of size (batch_size x max_seq_len)\n",
    "                containing the word indices of sentences in the batch\n",
    "    - lengths:  a vector of size (batch_size) containing the\n",
    "                original lengths of the sequences before padding\n",
    "\n",
    "    Output:\n",
    "    - logits:   a matrix of size (batch_size x max_seq_len x num_tags)\n",
    "                gives a score to each possible tag for each word\n",
    "                in each sentence \n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # cast arrays as PyTorch data types and move to GPU memory\n",
    "    indices = torch.LongTensor(indices).to(device)\n",
    "    lengths = torch.LongTensor(lengths).to(device)\n",
    "    \n",
    "    # convert word indices to word embeddings\n",
    "    embeddings = self.embeddings(indices)\n",
    "\n",
    "    # pack/pad handles variable length sequence batching\n",
    "    # see here if you're curious: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "    packed_input_embs = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "    # run input through LSTM layer\n",
    "    packed_output, _ = self.lstm(packed_input_embs)\n",
    "    # unpack sequences into original format\n",
    "    padded_output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "    logits = self.hidden2tag(padded_output)\n",
    "    return logits\n",
    "\n",
    "  def run_training(self, train_dataset, dev_dataset, batch_size, vocab, tagset,\n",
    "                         lr=5e-4, num_epochs=100, eval_every=5):\n",
    "    \"\"\"\n",
    "    Trains the model on the training data with a learning rate of lr\n",
    "    for num_epochs. Evaluates the model on the dev data eval_every epochs.\n",
    "\n",
    "    Arguments:\n",
    "    - train_dataset:  Dataset object containing the training data\n",
    "    - dev_dataset:    Dataset object containing the dev data\n",
    "    - batch_size:     batch size for train/dev data\n",
    "    - vocab:          a dictionary mapping word strings to indices\n",
    "    - tagset:         a dictionary mapping tag strings to indices\n",
    "    - lr:             learning rate\n",
    "    - num_epochs:     number of epochs to train for\n",
    "    - eval_every:     evaluation is run eval_every epochs\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     if str(device) == 'cpu':\n",
    "#       print(\"Training only supported in GPU environment\")\n",
    "#       return\n",
    "\n",
    "    # clear unreferenced data/models from GPU memory \n",
    "    torch.cuda.empty_cache()\n",
    "    # move model to GPU memory\n",
    "    self.to(device)\n",
    "\n",
    "    # set the optimizer (Adam) and loss function (CrossEnt)\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    # batch training and dev data\n",
    "    train_batch_idx, train_batch_tags, train_batch_lens = train_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "    dev_batch_idx, dev_batch_tags, dev_batch_lens = dev_dataset.get_batches(BATCH_SIZE, vocab, tagset)\n",
    "\n",
    "    print(\"**** TRAINING *****\")\n",
    "    for i in range(num_epochs):\n",
    "      # sets the model in train mode\n",
    "      self.train()\n",
    "\n",
    "      total_loss = 0\n",
    "      for b in range(len(train_batch_idx)):\n",
    "        # compute the logits\n",
    "        logits = self.forward(train_batch_idx[b], train_batch_lens[b])\n",
    "        # move labels to GPU memory\n",
    "        labels = torch.LongTensor(train_batch_tags[b]).to(device)\n",
    "        # compute the loss with respect to true labels\n",
    "        loss = loss_function(logits.view(-1, len(tagset)), labels.view(-1))\n",
    "        total_loss += loss\n",
    "        # propagate gradients backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set model gradients to zero before performing next forward pass\n",
    "        self.zero_grad()\n",
    "\n",
    "      print(\"Epoch {} | Loss: {}\".format(i, total_loss))\n",
    "\n",
    "      if (i + 1) % eval_every == 0:\n",
    "        print(\"**** EVALUATION *****\")\n",
    "        # sets the model in evaluate mode (no gradients)\n",
    "        self.eval()\n",
    "        # compute dev f1 score\n",
    "        acc, true, pred = self.evaluate(dev_batch_idx, dev_batch_lens, dev_batch_tags, tagset)\n",
    "        print(\"Dev Accuracy: {}\".format(acc))\n",
    "        print(\"**********************\")\n",
    "\n",
    "  def evaluate(self, batched_sentences, batched_lengths, batched_labels, tagset):\n",
    "    \"\"\"\n",
    "    Evaluate the model's predictions on the provided dataset. \n",
    "\n",
    "    Arguments:\n",
    "    - batched_sentences:  a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                          containing the word indices of sentences in the batch\n",
    "    - batched_lengths:    a list of vectors, each of size (batch_size), containing the\n",
    "                          original lengths of the sequences before padding\n",
    "    - batched_labels:     a list of matrices, each of size (batch_size x max_seq_len),\n",
    "                          containing the tag indices corresponding to sentences in the batch\n",
    "    - num_tags:           the number of possible output tags\n",
    "\n",
    "    Output:\n",
    "    - accuracy:           the model's prediction accuracy\n",
    "    - all_true_labels:    a flattened list of all true labels\n",
    "    - all_predictions:    a flattened list of all of the model's corresponding predictions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for b in range(len(batched_sentences)):\n",
    "      logits = self.forward(batched_sentences[b], batched_lengths[b])\n",
    "      batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "      batch_size, _ = batched_sentences[b].shape\n",
    "\n",
    "      for i in range(batch_size):\n",
    "        tags = batched_labels[b][i]\n",
    "        preds = batch_predictions[i]\n",
    "        \n",
    "        seq_len = int(batched_lengths[b][i])\n",
    "        for j in range(seq_len):\n",
    "          all_predictions.append(int(preds[j]))\n",
    "          all_true_labels.append(int(tags[j]))\n",
    "      \n",
    "    \n",
    "    acc = accuracy(all_true_labels, all_predictions)\n",
    "      \n",
    "    return acc, all_true_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** TRAINING *****\n",
      "Epoch 0 | Loss: 38.83001708984375\n",
      "Epoch 1 | Loss: 28.77641487121582\n",
      "Epoch 2 | Loss: 17.05821418762207\n",
      "Epoch 3 | Loss: 8.752432823181152\n",
      "Epoch 4 | Loss: 5.4473876953125\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9510744033037026\n",
      "**********************\n",
      "Epoch 5 | Loss: 4.354886054992676\n",
      "Epoch 6 | Loss: 3.892387866973877\n",
      "Epoch 7 | Loss: 3.6128294467926025\n",
      "Epoch 8 | Loss: 3.4044992923736572\n",
      "Epoch 9 | Loss: 3.2361550331115723\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9512143907048366\n",
      "**********************\n",
      "Epoch 10 | Loss: 3.0928428173065186\n",
      "Epoch 11 | Loss: 2.9660558700561523\n",
      "Epoch 12 | Loss: 2.8523731231689453\n",
      "Epoch 13 | Loss: 2.748305559158325\n",
      "Epoch 14 | Loss: 2.650189161300659\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9519143277105061\n",
      "**********************\n",
      "Epoch 15 | Loss: 2.5572893619537354\n",
      "Epoch 16 | Loss: 2.4691081047058105\n",
      "Epoch 17 | Loss: 2.3853631019592285\n",
      "Epoch 18 | Loss: 2.306419610977173\n",
      "Epoch 19 | Loss: 2.2306644916534424\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9516343529082383\n",
      "**********************\n",
      "Epoch 20 | Loss: 2.1582934856414795\n",
      "Epoch 21 | Loss: 2.08994460105896\n",
      "Epoch 22 | Loss: 2.025336265563965\n",
      "Epoch 23 | Loss: 1.963992953300476\n",
      "Epoch 24 | Loss: 1.9062587022781372\n",
      "**** EVALUATION *****\n",
      "Dev Accuracy: 0.9512143907048366\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "# intialize a new LSTMTagger model\n",
    "lstm_model = LSTMTagger(embeddings, HIDDEN_SIZE, len(tagset))\n",
    "# train the model\n",
    "lstm_model.run_training(train_dataset, dev_dataset, BATCH_SIZE, vocab, tagset,   \n",
    "                   lr=5e-4, num_epochs=25, eval_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************\n",
      "TAG: B-PER\n",
      "(88 pred, 328 true)\n",
      "PRECISION: \t0.341\n",
      "RECALL: \t0.091\n",
      "F1 SCORE: \t0.144\n",
      "***********************\n",
      "TAG: I-PER\n",
      "(6 pred, 168 true)\n",
      "PRECISION: \t0.333\n",
      "RECALL: \t0.012\n",
      "F1 SCORE: \t0.023\n",
      "***********************\n",
      "TAG: B-NORP\n",
      "(0 pred, 14 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-NORP\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-FAC\n",
      "(0 pred, 7 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-FAC\n",
      "(0 pred, 20 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-ORG\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-ORG\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-GPE\n",
      "(0 pred, 33 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-GPE\n",
      "(0 pred, 30 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-LOC\n",
      "(0 pred, 36 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-LOC\n",
      "(0 pred, 18 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-PRODUCT\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-PRODUCT\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-EVENT\n",
      "(0 pred, 2 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-EVENT\n",
      "(0 pred, 4 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-ART\n",
      "(0 pred, 12 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-ART\n",
      "(0 pred, 8 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-LAW\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-LAW\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: B-LANGUAGE\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: I-LANGUAGE\n",
      "(0 pred, 0 true)\n",
      "PRECISION: \t0.000\n",
      "RECALL: \t0.000\n",
      "F1 SCORE: \t0.000\n",
      "***********************\n",
      "TAG: O\n",
      "(14193 pred, 13595 true)\n",
      "PRECISION: \t0.955\n",
      "RECALL: \t0.997\n",
      "F1 SCORE: \t0.976\n"
     ]
    }
   ],
   "source": [
    "eval_per_class(lstm_model, dev_dataset, vocab, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
